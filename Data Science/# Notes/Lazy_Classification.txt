Lazy learners simply store the training data and wait until a testing data appear. When it does, classification is
conducted based on the most related data in the stored training data. Compared to eager learners, lazy learners have
less training time but more time in predicting.

Ex. k-nearest neighbor, Case-based reasoning

Why is Nearest Neighbor a Lazy Algorithm? (https://sebastianraschka.com/faq/docs/lazy-knn.html)
-----------------------------------------
Although, Nearest neighbor algorithms, for instance, the K-Nearest Neighbors (K-NN) for classification,
are very “simple” algorithms, that’s not why they are called lazy ;). K-NN is a lazy learner because it
doesn’t learn a discriminative function from the training data but “memorizes” the training dataset instead.

For example, the logistic regression algorithm learns its model weights (parameters) during training time.
In contrast, there is no training time in K-NN. Although this may sound very convenient, this property doesn’t
come without a cost: The “prediction” step in K-NN is relatively expensive! Each time we want to make a prediction,
K-NN is searching for the nearest neighbor(s) in the entire training set! (Note that there are certain tricks such as
BallTrees and KDtrees to speed this up a bit.)

To summarize: An eager learner has a model fitting or training step. A lazy learner does not have a training phase.


Motivation: (https://en.wikipedia.org/wiki/Lazy_learning)
-----------
The primary motivation for employing lazy learning, as in the K-nearest neighbors algorithm, used by online
recommendation systems ("people who viewed/purchased/listened to this movie/item/tune also ...") is that the
data set is continuously updated with new entries (e.g., new items for sale at Amazon, new movies to view at Netflix,
new clips at Youtube, new music at Spotify or Pandora). Because of the continuous update, the "training data" would be
rendered obsolete in a relatively short time especially in areas like books and movies, where new best-sellers or hit
movies/music are published/released continuously. Therefore, one cannot really talk of a "training phase"


Examples of Lazy Learning Methods (https://en.wikipedia.org/wiki/Lazy_learning)
---------------------------------
- K-nearest neighbors, which is a special case of instance-based learning.
- Local regression.
- Lazy naive Bayes rules, which are extensively used in commercial spam detection software.
Here, the spammers keep getting smarter and revising their spamming strategies, and therefore the learning rules
must also be continually updated.