<html>
    <head>
        <title>Deep Learning Libraries</title>
    </head>
    <body>
        <h1>Deep Learning Libraries</h1>
        <table border="2">
            <tr>
                <th>Name</th>
                <th>Date since created</th>
                <th>Properties</th>
                <th>Popularity</th>
                <th>Notes</th>
            </tr>

            <tr>
                <td>1 - Caffe</td>
                <td>April 2017 (17 months) <a href="https://en.wikipedia.org/wiki/Caffe_(software)">Link</a></td>
                <td>
                    <ul>
                        <li>supports many different types of deep learning architectures geared towards image classification and image segmentation.</li>
                        <li>supports CNN, RCNN, LSTM and fully connected neural network designs.</li>
                        <li>supports GPU- and CPU-based acceleration computational kernel libraries.</li>
                    </ul>
                </td>
                <td>25,000+ github stars</td>
                <td>In April 2017, Facebook announced Caffe2,[12] which includes new features such as Recurrent Neural Networks. At the end of March 2018, Caffe2 was merged into PyTorch.</td>
            </tr>

            <tr>
                <td>2 - Apache MXNet</td>
                <td>January 2017 <a href="https://mapr.com/blog/tensorflow-mxnet-caffe-h2o-which-ml-best/">Link</a></td>
                <td>
                    <ul>
                        <li>Design notes providing useful insights that can re-used by other DL projects.</li>
                        <li>Flexible configuration for arbitrary computation graph.</li>
                        <li>Mix and match imperative and symbolic programming to maximize flexibility and efficiency.</li>
                        <li>Lightweight, memory efficient and portable to smart devices.</li>
                        <li>Scales up to multi GPUs and distributed setting with auto parallelism.</li>
                        <li>Support for Python, R, Scala, C++ and Julia.</li>
                        <li>Cloud-friendly and directly compatible with S3, HDFS, and Azure.</li>
                    </ul>
                </td>
                <td>15,000+ github stars</td>
                <td>MXNet is also more than a deep learning project. It is also a collection of blue prints and guidelines for building deep learning systems, and interesting insights of DL systems for hackers.</td>
            </tr>

            <tr>
                <td>3 - PyTorch</td>
                <td>
                    <ul>
                        <li>Initial release: October 2016</li>
                        <li>Stable release: July 2018</li>
                        <a href="https://en.wikipedia.org/wiki/PyTorch">Link</a>
                    </ul>
                </td>
                <td>
                    <ul>
                        <li>Tensor computation (like NumPy) with strong GPU acceleration.</li>
                        <li>Deep neural networks built on a tape-based autograd system.</li>
                        <li>Has minimal framework overhead.</li>
                        <li>PyTorch is designed to be intuitive, linear in thought and easy to use. When you execute a line of code, it gets executed. There isn't an asynchronous view of the world.</li>
                    </ul>
                </td>
                <td>20,000+ github stars</td>
                <td>PyTorch Tensors are nothing but multidimensional arrays. Tensors in PyTorch are similar to NumPy arrays, with the addition being that Tensors can also be used on a GPU that supports CUDA. PyTorch supports various types of Tensors.</td>
            </tr>

            <tr>
                <td>4 - TensorFlow</td>
                <td>
                    <ul>
                        <li>Initial release: November 2015</li>
                        <li>Stable release:  September 2018</li>
                        <a href="https://en.wikipedia.org/wiki/TensorFlow">Link</a>
                    </ul>
                </td>
                <td>
                    <ul>
                        <li>High performance numerical computation.</li>
                        <li>Its flexible architecture allows easy deployment of computation across a variety of platforms (CPUs, GPUs, TPUs).</li>
                    </ul>
                </td>
                <td>112,000+ github stars</td>
                <td>TensorFlow was originally developed by researchers and engineers working on the Google Brain team within Google's Machine Intelligence Research organization for the purposes of conducting machine learning and deep neural networks research. The system is general enough to be applicable in a wide variety of other domains, as well.</td>
            </tr>

            <tr>
                <td>5 - NeuPy</td>
                <td>around 1-2 years</td>
                <td>
                    <ul>
                        <li>Deep Learning</li>
                        <li>Reinforcement Learning (RL)</li>
                        <li>Convolutional Neural Networks (CNN)</li>
                        <li>Recurrent Neural Networks (RNN)</li>
                        <li>Restricted Boltzmann Machine (RBM)</li>
                        <li>Multilayer Perceptron (MLP)</li>
                        <li>Networks based on the Radial Basis Functions (RBFN)</li>
                        <li>Associative and Autoasociative Memory</li>
                        <li>Ensemble Networks</li>
                        <li>Competitive Networks</li>
                        <li>Basic Linear Networks</li>
                        <li>Regularization Algorithms</li>
                        <li>Step Update Algorithms</li>
                    </ul>
                </td>
                <td>492 github stars</td>
                <td>NeuPy will use Tensorflow as a backend starting from version 0.7.0</td>
            </tr>

            <tr>
                <td>6 - Keras</td>
                <td>
                    <ul>
                        <li>Initial release: March 2015</li>
                        <li>Stable release:  June 2018</li>
                        <a href="https://en.wikipedia.org/wiki/Keras">Link</a>
                    </ul>
                </td>
                <td>
                    <ul>
                        <li>Allows for easy and fast prototyping (through user friendliness, modularity, and extensibility).</li>
                        <li>Supports both convolutional networks and recurrent networks, as well as combinations of the two.</li>
                        <li>Runs seamlessly on CPU and GPU.</li>
                    </ul>
                </td>
                <td>34,000+ github stars</td>
                <td>Keras is a high-level neural networks API, written in Python and capable of running on top of TensorFlow, CNTK, or Theano. It was developed with a focus on enabling fast experimentation. Being able to go from idea to result with the least possible delay is key to doing good research.</td>
            </tr>

        </table>
    </body>
</html>